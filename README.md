# k8s-gpu-mcp-server

**Just-in-Time SRE Diagnostic Agent for NVIDIA GPU Clusters on Kubernetes**

[![Go Version](https://img.shields.io/badge/Go-1.25+-00ADD8?style=flat&logo=go)](https://go.dev/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![CI Status](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/workflows/CI/badge.svg)](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/actions)
[![GitHub Issues](https://img.shields.io/github/issues/ArangoGutierrez/k8s-gpu-mcp-server)](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/issues)
[![GitHub Stars](https://img.shields.io/github/stars/ArangoGutierrez/k8s-gpu-mcp-server?style=social)](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server)
[![MCP](https://img.shields.io/badge/MCP-2025--06--18-purple)](https://modelcontextprotocol.io/)

---

## Overview

`k8s-gpu-mcp-server` is an **ephemeral diagnostic agent** that provides surgical, 
real-time NVIDIA GPU hardware introspection for Kubernetes clusters via the 
[Model Context Protocol (MCP)](https://modelcontextprotocol.io/). 

Unlike traditional monitoring systems, this agent is designed for **AI-assisted 
troubleshooting** by SREs debugging complex hardware failures that standard 
Kubernetes APIs cannot detect.

### âœ¨ Key Features

- ğŸ¯ **Low Footprint** - Persistent HTTP server with ~15-20MB memory when idle
- ğŸ”Œ **HTTP Transport** - JSON-RPC 2.0 over HTTP/SSE (production default)
- ğŸ” **Deep Hardware Access** - Direct NVML integration for GPU diagnostics
- ğŸ¤– **AI-Native** - Built for Claude Desktop, Cursor, and MCP-compatible hosts
- ğŸ”’ **Secure by Default** - Read-only operations with explicit operator mode
- âš¡ **Production Ready** - Real Tesla T4 testing, 538 tests passing

---

## ğŸš€ Quick Start

### One-Click Install

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=k8s-gpu-mcp&config=eyJtY3BTZXJ2ZXJzIjp7Ims4cy1ncHUtbWNwIjp7ImNvbW1hbmQiOiJucHgiLCJhcmdzIjpbIi15IiwiazhzLWdwdS1tY3Atc2VydmVyQGxhdGVzdCJdfX19Cg==)

Click the button above to install automatically in Cursor.

### One-Line Installation

```bash
# Using npx (recommended)
npx k8s-gpu-mcp-server@latest

# Or install globally
npm install -g k8s-gpu-mcp-server
```

<details>
<summary><strong>ğŸ“‹ Manual Configuration: Cursor / VS Code</strong></summary>

Add to `~/.cursor/mcp.json` (Cursor) or VS Code MCP config:

```json
{
  "mcpServers": {
    "k8s-gpu-mcp": {
      "command": "npx",
      "args": ["-y", "k8s-gpu-mcp-server@latest"]
    }
  }
}
```

</details>

<details>
<summary><strong>ğŸ“‹ Manual Configuration: Claude Desktop</strong></summary>

**macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`  
**Windows:** `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "k8s-gpu-mcp": {
      "command": "npx",
      "args": ["-y", "k8s-gpu-mcp-server@latest"]
    }
  }
}
```

</details>

### Install from Source

```bash
# Clone and build
git clone https://github.com/ArangoGutierrez/k8s-gpu-mcp-server.git
cd k8s-gpu-mcp-server
make agent

# Test with mock GPUs (no hardware required)
cat examples/gpu_inventory.json | ./bin/agent --nvml-mode=mock

# Test with real GPU (requires NVIDIA driver)
cat examples/gpu_inventory.json | ./bin/agent --nvml-mode=real
```

### Deploy to Kubernetes

```bash
# Deploy with Helm (RuntimeClass mode - recommended)
helm install k8s-gpu-mcp-server ./deployment/helm/k8s-gpu-mcp-server \
  --namespace gpu-diagnostics --create-namespace

# Find agent pod on target node
NODE_NAME=<node-name>
POD=$(kubectl get pods -n gpu-diagnostics \
  -l app.kubernetes.io/name=k8s-gpu-mcp-server \
  --field-selector spec.nodeName=$NODE_NAME \
  -o jsonpath='{.items[0].metadata.name}')

# Start diagnostic session
kubectl exec -it -n gpu-diagnostics $POD -- /agent --mode=read-only
```

> **Note:** GPU access requires `runtimeClassName: nvidia` configured by
> GPU Operator or nvidia-ctk. For clusters without RuntimeClass, use fallback:
> `--set gpu.runtimeClass.enabled=false --set gpu.resourceRequest.enabled=true`

### Configure Claude Desktop with kubectl (Advanced)

For deployed agents, add to your Claude Desktop configuration:

```json
{
  "mcpServers": {
    "k8s-gpu-agent": {
      "command": "kubectl",
      "args": ["exec", "-i", "deploy/k8s-gpu-mcp-server", "-n", "gpu-diagnostics", "--", "/agent"]
    }
  }
}
```

Then ask Claude: *"What's the temperature of the GPUs?"*

ğŸ“– **[Full Quick Start Guide â†’](docs/quickstart.md)**

---

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MCP Client (Claude/Cursor)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ stdio / HTTP
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Gateway Pod (:8080)                               â”‚
â”‚       Router â†’ Circuit Breaker â†’ HTTP Client                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP (pod-to-pod)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent (Node 1) â”‚  â”‚  Agent (Node 2) â”‚  â”‚  Agent (Node N) â”‚
â”‚  5 MCP Tools    â”‚  â”‚  5 MCP Tools    â”‚  â”‚  5 MCP Tools    â”‚
â”‚  NVML â†’ GPU     â”‚  â”‚  NVML â†’ GPU     â”‚  â”‚  NVML â†’ GPU     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Design Principles:**
- **HTTP-First**: Gateway routes via HTTP to agent pods (~50ms latency)
- **Low Footprint**: Persistent HTTP server, ~15-20MB memory
- **Observable**: Circuit breaker, Prometheus metrics, distributed tracing
- **Interface Abstraction**: Testable, flexible, portable (538 tests)

ğŸ“– **[Architecture Documentation â†’](docs/architecture.md)**

---

## ğŸ› ï¸ Available Tools

| Tool | Description | Status |
|------|-------------|--------|
| `get_gpu_inventory` | Hardware inventory + telemetry | âœ… Available |
| `get_gpu_health` | GPU health monitoring with scoring | âœ… Available |
| `analyze_xid_errors` | Parse GPU XID error codes from kernel logs | âœ… Available |
| `describe_gpu_node` | Node-level GPU diagnostics with K8s metadata | âœ… Available |
| `get_pod_gpu_allocation` | GPU-to-Pod correlation via resource requests | âœ… Available |
| `kill_gpu_process` | Terminate GPU process | ğŸš§ M4 (Operator) |
| `reset_gpu` | GPU reset | ğŸš§ M4 (Operator) |

ğŸ“– **[MCP Usage Guide â†’](docs/mcp-usage.md)**

---

## ğŸ“ˆ Project Status

### Current Milestone: [M3: Kubernetes Integration](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/milestone/3)
**Progress:** ~90% Complete (HTTP Transport âœ…, Gateway âœ…, K8s Tools âœ…)

### Completed Milestones
- âœ… [M1: Foundation & API](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/milestone/1) - Completed Jan 3, 2026
- âœ… [M2: Hardware Introspection](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/milestone/2) - Completed Jan 10, 2026
  - Real NVML integration, tested on Tesla T4
  - GPU health monitoring, XID error analysis
  - npm/Helm distribution

### Recent Updates (Jan 2026)
- **Jan 16**: Documentation 360 review for external contributors
- **Jan 15**: K8s tools complete (`describe_gpu_node`, `get_pod_gpu_allocation`)
- **Jan 14**: HTTP Transport Epic complete - 150Ã— latency improvement
- **Jan 14**: Cross-node networking fix (Calico VXLAN)
- **Jan 13**: Gateway mode with circuit breaker & Prometheus metrics

ğŸ“Š **[View All Milestones â†’](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/milestones)**

---

## ğŸ§ª Testing

### Unit Tests (No GPU Required)

```bash
make test                   # Run all unit tests (538 tests passing)
make coverage               # Generate coverage report
make coverage-html          # View coverage in browser
```

### Integration Tests (Requires GPU)

```bash
make test-integration       # Run on GPU hardware
# Or manually:
go test -tags=integration -v ./pkg/nvml/
```

**Latest Test Results:**
```
âœ“ 538 total tests passing
âœ“ Race detector enabled (-race)
âœ“ Coverage: 58-80% by package

Integration tested on Tesla T4:
  - GPU: Tesla T4 (15GB)
  - Temperature: 29Â°C
  - Power: 13.9W
  - All NVML operations verified
```

---

## ğŸ—ï¸ Build

```bash
# Build for local platform
make agent

# Build for Linux (with real NVML)
CGO_ENABLED=1 GOOS=linux GOARCH=amd64 make agent

# Build container image
make image

# Multi-arch release builds
make dist
```

**Binary Sizes:**
- Mock mode: **4.3MB** (CGO disabled)
- Real mode: **7.9MB** (CGO enabled)

---

## ğŸ“¦ Installation

### Using npm (Recommended)

```bash
# Run directly with npx
npx k8s-gpu-mcp-server@latest

# Or install globally
npm install -g k8s-gpu-mcp-server
```

### From Source

```bash
git clone https://github.com/ArangoGutierrez/k8s-gpu-mcp-server.git
cd k8s-gpu-mcp-server
make agent
sudo mv bin/agent /usr/local/bin/k8s-gpu-mcp-server
```

### Using Go

```bash
go install github.com/ArangoGutierrez/k8s-gpu-mcp-server/cmd/agent@latest
```

### Container Image (Coming in M3)

```bash
docker pull ghcr.io/arangogutierrez/k8s-gpu-mcp-server:latest
```

---

## ğŸ¤ Contributing

We welcome contributions! Please see our [Development Guide](DEVELOPMENT.md)
for details.

### Quick Contribution Guide

1. Check [open issues](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/issues)
2. Fork and create feature branch: `git checkout -b feat/my-feature`
3. Make changes, add tests
4. Run checks: `make all`
5. Commit with DCO: `git commit -s -S -m "feat(scope): description"`
6. Open PR with labels and milestone

ğŸ“– **[Full Development Guide â†’](DEVELOPMENT.md)**

---

## ğŸ“š Documentation

- **[Quick Start Guide](docs/quickstart.md)** - Get running in 5 minutes
- **[Architecture](docs/architecture.md)** - System design and components
- **[MCP Usage](docs/mcp-usage.md)** - How to consume the MCP server
- **[Development Guide](DEVELOPMENT.md)** - Contributing guidelines
- **[Examples](examples/)** - Sample JSON-RPC requests

---

## ğŸ”§ Technology Stack

- **Language**: Go 1.25+ (latest stable)
- **MCP Protocol**: [mcp-go v0.43.2](https://github.com/mark3labs/mcp-go)
- **GPU Library**: [go-nvml v0.13.0-1](https://github.com/NVIDIA/go-nvml)
- **Testing**: [testify v1.10.0](https://github.com/stretchr/testify)
- **Container**: Distroless Debian 12 (coming in M3)

---

## ğŸ¯ Use Cases

### 1. Debugging Stuck Training Jobs

```
SRE: "Why is the training job on node-5 stuck?"
Claude â†’ k8s-gpu-mcp-server â†’ Detects XID 48 (ECC Error)
Claude: "Node-5 has uncorrectable memory errors. Drain immediately."
```

### 2. Thermal Management

```
SRE: "Are any GPUs thermal throttling?"
Claude â†’ k8s-gpu-mcp-server â†’ Checks temps and throttle status
Claude: "GPU 3 is at 86Â°C and thermal throttling. Check cooling."
```

### 3. Topology Validation

```
SRE: "Is NVLink properly configured for multi-GPU training?"
Claude â†’ k8s-gpu-mcp-server â†’ Inspects NVLink topology
Claude: "All 8 GPUs connected via NVLink, 600GB/s bandwidth."
```

### 4. Zombie Process Hunting

```
SRE: "GPU memory is full but no pods are running"
Claude â†’ k8s-gpu-mcp-server â†’ Lists GPU processes
Claude: "Found zombie process PID 12345 using 8GB. Kill it?"
```

---

## ğŸ† Achievements

- âœ… **Go 1.25** - Latest Go version
- âœ… **Real NVML** - Tested on Tesla T4
- âœ… **538 Tests Passing** - Race detector enabled, 58-80% coverage
- âœ… **HTTP-First Architecture** - 150Ã— faster than exec routing
- âœ… **Gateway + Circuit Breaker** - Production-grade reliability
- âœ… **Prometheus Metrics** - Per-node latency tracking
- âœ… **~8MB Binary** - 84% under 50MB target
- âœ… **MCP 2025-06-18** - Latest protocol version

---

## ğŸ“„ License

Apache License 2.0 - See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- [NVIDIA NVML](https://developer.nvidia.com/nvidia-management-library-nvml) - GPU Management Library
- [Model Context Protocol](https://modelcontextprotocol.io/) - MCP Specification
- [mcp-go](https://github.com/mark3labs/mcp-go) - MCP Go Implementation
- [Anthropic Claude](https://www.anthropic.com/claude) - AI Assistant
- [Cursor](https://cursor.sh/) - AI-Powered IDE

---

## ğŸ“ Contact

**Maintainer:** [@ArangoGutierrez](https://github.com/ArangoGutierrez)  
**Issues:** [GitHub Issues](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/issues)  
**Discussions:** [GitHub Discussions](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/discussions)

---

<div align="center">

**â­ Star us on GitHub â€” it helps!**

[Report Bug](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/issues/new?template=bug_report.yml) Â· 
[Request Feature](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/issues/new?template=feature_request.yml) Â· 
[View Roadmap](https://github.com/ArangoGutierrez/k8s-gpu-mcp-server/milestones)

</div>
