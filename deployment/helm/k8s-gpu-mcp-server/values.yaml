# Copyright 2026 k8s-gpu-mcp-server contributors
# SPDX-License-Identifier: Apache-2.0

# Default values for k8s-gpu-mcp-server Helm chart.

# -- Override the name of the chart
nameOverride: ""

# Agent configuration
agent:
  # -- Operation mode: read-only or operator
  mode: "read-only"
# -- Override the full name of the chart
fullnameOverride: ""

# Namespace configuration
namespace:
  # -- Create a dedicated namespace for the agent
  create: true
  # -- Namespace name (used if create is true or for resource deployment)
  name: "gpu-diagnostics"

# Image configuration
image:
  # -- Container image repository
  repository: ghcr.io/arangogutierrez/k8s-gpu-mcp-server
  # -- Image pull policy
  pullPolicy: IfNotPresent
  # -- Overrides the image tag (default is chart appVersion)
  tag: ""

# -- Image pull secrets for private registries
imagePullSecrets: []

# ServiceAccount configuration
serviceAccount:
  # -- Create a ServiceAccount
  create: true
  # -- ServiceAccount name (generated if not set)
  name: ""
  # -- Annotations to add to the ServiceAccount
  annotations: {}

# GPU access configuration
gpu:
  # RuntimeClass-based GPU access (recommended)
  # Requires: nvidia RuntimeClass configured via GPU Operator or nvidia-ctk
  runtimeClass:
    # -- Enable RuntimeClass for GPU access (recommended)
    # When enabled, uses nvidia-container-runtime via RuntimeClass for CDI injection
    enabled: true
    # -- Name of the RuntimeClass to use
    name: "nvidia"

  # Device Plugin resource request (fallback for clusters without RuntimeClass)
  # Uses traditional resources.limits mechanism
  resourceRequest:
    # -- Enable GPU resource request from device plugin
    # WARNING: This consumes nvidia.com/gpu resources and may block scheduler
    # Only enable if RuntimeClass is not configured in your cluster
    enabled: false
    # -- GPU resource name (nvidia.com/gpu for device plugin)
    resource: "nvidia.com/gpu"
    # -- Number of GPUs to request (typically 1 for monitoring all GPUs)
    count: 1

  # Dynamic Resource Allocation (DRA) - Kubernetes 1.26+, beta in 1.31+
  # Uses ResourceClaim mechanism for fine-grained GPU allocation
  # Requires: NVIDIA DRA Driver deployed in cluster
  resourceClaim:
    # -- Enable DRA-based GPU access
    # Mutually exclusive with resourceRequest.enabled
    enabled: false
    # -- Name for the resource claim reference
    name: "gpu"
    # -- Reference an existing ResourceClaimTemplate by name
    # If set, uses resourceClaimTemplateName in pod spec
    templateName: ""
    # -- Create an inline ResourceClaim spec (advanced)
    # If templateName is empty and spec is provided, uses inline claim
    # Example:
    #   spec:
    #     devices:
    #       requests:
    #       - name: gpu
    #         deviceClassName: gpu.nvidia.com
    spec: {}

# Node selection
# -- Node selector for GPU nodes (empty by default, runs on all nodes)
# Examples:
#   nvidia.com/gpu.count: "1"        # GPU Feature Discovery label
#   nvidia.com/gpu.present: "true"   # Manual labeling
nodeSelector: {}

# Tolerations for GPU nodes
tolerations:
  # -- Tolerate GPU node taints
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Pod security configuration
securityContext:
  # -- Run as root (may be required for NVML access)
  runAsUser: 0
  # -- Prevent privilege escalation
  allowPrivilegeEscalation: false
  # -- Read-only root filesystem
  readOnlyRootFilesystem: true
  # -- Drop all capabilities (add only if needed)
  capabilities:
    drop:
      - ALL
    # CAP_SYSLOG required for /dev/kmsg access (XID error analysis)
    # SYS_ADMIN required for profiling metrics (like dcgm-exporter)
    add:
      - SYSLOG

# Resource limits
resources:
  requests:
    cpu: 1m
    memory: 10Mi
  limits:
    cpu: 100m
    memory: 50Mi

# Pod annotations
podAnnotations: {}

# Pod labels
podLabels: {}

# -- Priority class name for the DaemonSet pods
priorityClassName: ""

# Update strategy
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1

# Transport configuration
transport:
  # -- Transport mode: stdio or http
  # stdio: Uses kubectl exec for interaction (default)
  # http: Exposes HTTP endpoint for MCP access
  mode: stdio

  # HTTP mode settings (only used if mode: http)
  http:
    # -- HTTP port to listen on
    port: 8080
    # -- HTTP listen address
    addr: "0.0.0.0"

# Service configuration (for HTTP mode)
service:
  # -- Enable service (required for HTTP mode)
  enabled: false
  # -- Service type
  type: ClusterIP
  # -- Service port
  port: 8080

# XID error analysis configuration
xidAnalysis:
  # -- Enable /dev/kmsg mount for XID error detection
  # Required for analyze_xid_errors tool in distroless containers
  # Note: Also requires privileged: true in securityContext to read /dev/kmsg
  enabled: true

# Gateway configuration
# Gateway provides a single MCP entry point for multi-node GPU clusters
gateway:
  # -- Enable gateway deployment
  # When enabled, deploys a gateway that routes requests to node agents
  enabled: false

  # -- Number of gateway replicas
  replicas: 1

  # -- Gateway HTTP port
  port: 8080

  # -- Gateway service configuration
  service:
    # -- Service type for gateway
    type: ClusterIP
    # -- Service port (defaults to gateway.port)
    port: 8080

  # -- Resource limits for gateway pods
  resources:
    requests:
      cpu: 10m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 128Mi

