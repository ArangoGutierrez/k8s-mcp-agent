# Copyright 2026 k8s-gpu-mcp-server contributors
# SPDX-License-Identifier: Apache-2.0

# Default values for k8s-gpu-mcp-server Helm chart.

# -- Override the name of the chart
nameOverride: ""

# Agent configuration
agent:
  # -- Operation mode: read-only or operator
  mode: "read-only"
# -- Override the full name of the chart
fullnameOverride: ""

# Namespace configuration
namespace:
  # -- Create a dedicated namespace for the agent
  create: true
  # -- Namespace name (used if create is true or for resource deployment)
  name: "gpu-diagnostics"

# Image configuration
image:
  # -- Container image repository
  repository: ghcr.io/arangogutierrez/k8s-gpu-mcp-server
  # -- Image pull policy
  pullPolicy: IfNotPresent
  # -- Overrides the image tag (default is chart appVersion)
  tag: ""

# -- Image pull secrets for private registries
imagePullSecrets: []

# ServiceAccount configuration
serviceAccount:
  # -- Create a ServiceAccount
  create: true
  # -- ServiceAccount name (generated if not set)
  name: ""
  # -- Annotations to add to the ServiceAccount
  annotations: {}

# GPU access configuration
gpu:
  # RuntimeClass-based GPU access (recommended)
  # Requires: nvidia RuntimeClass configured via GPU Operator or nvidia-ctk
  runtimeClass:
    # -- Enable RuntimeClass for GPU access (recommended)
    # When enabled, uses nvidia-container-runtime via RuntimeClass for CDI injection
    enabled: true
    # -- Name of the RuntimeClass to use
    name: "nvidia"

  # Device Plugin resource request (fallback for clusters without RuntimeClass)
  # Uses traditional resources.limits mechanism
  resourceRequest:
    # -- Enable GPU resource request from device plugin
    # WARNING: This consumes nvidia.com/gpu resources and may block scheduler
    # Only enable if RuntimeClass is not configured in your cluster
    enabled: false
    # -- GPU resource name (nvidia.com/gpu for device plugin)
    resource: "nvidia.com/gpu"
    # -- Number of GPUs to request (typically 1 for monitoring all GPUs)
    count: 1

  # Dynamic Resource Allocation (DRA) - Kubernetes 1.26+, beta in 1.31+
  # Uses ResourceClaim mechanism for fine-grained GPU allocation
  # Requires: NVIDIA DRA Driver deployed in cluster
  resourceClaim:
    # -- Enable DRA-based GPU access
    # Mutually exclusive with resourceRequest.enabled
    enabled: false
    # -- Name for the resource claim reference
    name: "gpu"
    # -- Reference an existing ResourceClaimTemplate by name
    # If set, uses resourceClaimTemplateName in pod spec
    templateName: ""
    # -- Create an inline ResourceClaim spec (advanced)
    # If templateName is empty and spec is provided, uses inline claim
    # Example:
    #   spec:
    #     devices:
    #       requests:
    #       - name: gpu
    #         deviceClassName: gpu.nvidia.com
    spec: {}

# Node selection
# -- Node selector for GPU nodes (empty by default, runs on all nodes)
# Examples:
#   nvidia.com/gpu.count: "1"        # GPU Feature Discovery label
#   nvidia.com/gpu.present: "true"   # Manual labeling
nodeSelector: {}

# Tolerations for GPU nodes
tolerations:
  # -- Tolerate GPU node taints
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Pod security configuration
securityContext:
  # -- Run as root (may be required for NVML access)
  runAsUser: 0
  # -- Prevent privilege escalation
  allowPrivilegeEscalation: false
  # -- Read-only root filesystem
  readOnlyRootFilesystem: true
  # -- Drop all capabilities (add only if needed)
  capabilities:
    drop:
      - ALL
    # CAP_SYSLOG required for /dev/kmsg access (XID error analysis)
    # SYS_ADMIN required for profiling metrics (like dcgm-exporter)
    add:
      - SYSLOG

# Resource limits
# Note: HTTP mode requires more baseline memory since agent stays resident
# Oneshot/stdio mode has lower baseline but higher peak during requests
resources:
  requests:
    cpu: 10m
    memory: 32Mi
  limits:
    cpu: 100m
    memory: 128Mi

# Pod annotations
podAnnotations: {}

# Pod labels
podLabels: {}

# -- Priority class name for the DaemonSet pods
priorityClassName: ""

# Update strategy
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1

# Transport configuration
#
# HTTP Mode (default, recommended for production):
#   - Agent runs as persistent HTTP server
#   - NVML initialized once at startup (~50-100ms)
#   - Per-request latency: 12-57ms
#   - Memory: constant ~15-20MB
#   - Use case: Gateway routing, high-frequency queries
#
# Stdio Mode (legacy, for debugging):
#   - Agent spawned per kubectl exec
#   - NVML initialized per request
#   - Per-request latency: 130-290ms
#   - Memory: spiky (5MB idle â†’ 200MB peak)
#   - Use case: Direct kubectl debugging, SRE access
transport:
  # -- Transport mode: http (recommended) or stdio (legacy)
  mode: http

  # HTTP mode settings (only used if mode: http)
  http:
    # -- HTTP port to listen on
    port: 8080
    # -- HTTP listen address
    addr: "0.0.0.0"

# Service configuration (for HTTP mode)
service:
  # -- Enable service for agent pods (should be enabled when transport.mode=http)
  # Required for gateway to discover agent endpoints
  enabled: true
  # -- Service type
  type: ClusterIP
  # -- Service port
  port: 8080
  # -- Create headless service (clusterIP: None) for DNS-based pod routing.
  # When enabled, each agent pod gets a stable DNS A record:
  #   <pod-name>.<service-name>.<namespace>.svc.cluster.local
  # This is useful when:
  #   - transport.mode=http and gateway.enabled=true
  #   - You need deterministic, DNS-based routing to individual agents
  #   - Pod IPs change frequently (e.g., spot instances)
  # Note: Pod IP routing (the default) is preferred when CNI is working.
  # See docs/troubleshooting/cross-node-networking.md for details.
  headless: false

# XID error analysis configuration
xidAnalysis:
  # -- Enable /dev/kmsg mount for XID error detection
  # Required for analyze_xid_errors tool in distroless containers
  # Note: Also requires privileged: true in securityContext to read /dev/kmsg
  enabled: true

# Gateway configuration
# Gateway provides a single MCP entry point for multi-node GPU clusters
gateway:
  # -- Enable gateway deployment
  # When enabled, deploys a gateway that routes requests to node agents
  enabled: false

  # -- Number of gateway replicas
  replicas: 1

  # -- Gateway HTTP port
  port: 8080

  # -- Routing mode for agent communication
  # http: Direct HTTP to agent pods (recommended, requires transport.mode=http)
  # exec: kubectl exec to agent pods (legacy, works with any transport.mode)
  routingMode: http

  # -- Timeout for kubectl exec operations to agent pods (only used in exec mode).
  # Must be less than HTTP WriteTimeout (90s) to prevent race conditions.
  execTimeout: "60s"

  # -- Gateway service configuration
  service:
    # -- Service type for gateway
    type: ClusterIP
    # -- Service port (defaults to gateway.port)
    port: 8080

  # -- Resource limits for gateway pods
  resources:
    requests:
      cpu: 10m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 128Mi

# NetworkPolicy configuration
# Controls pod-to-pod communication security
networkPolicy:
  # -- Enable NetworkPolicy for pod-to-pod communication security
  enabled: false  # -- Allow Prometheus to scrape agent metrics
  allowPrometheus: false  # -- Namespace selector for Prometheus (if allowPrometheus: true)
  prometheusNamespaceSelector:
    kubernetes.io/metadata.name: monitoring
